# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['lazy_system_p', 'LazyState', 'LLM', 'LazyEvaluationClient']

# %% ../nbs/00_core.ipynb 3
from dotenv import load_dotenv
import os
from anthropic import AnthropicVertex
from anthropic.types import (
    MessageParam,
    Message
)
from fastcore.basics import *
from fastcore.test import *
from fastcore.foundation import *
from dataclasses import (
    dataclass,
    field
)
from typing import (
    List,
    Optional
)
from nbdev.showdoc import show_doc

# %% ../nbs/00_core.ipynb 7
@dataclass
class LazyState:
    problem: str
    steps: List[str] = field(default_factory=list)
    current_step: int = 0

    def __post_init__(self):
        self.steps.append(self.problem)

    def add_step(self, step: str):
        self.steps.append(step)
        self.current_step += 1

    def get_context(self) -> str:
        return f"Problem: {self.problem} \n Steps so far: {self.steps}"

    def refresh(self) -> None:
        self.current_step = 0
        self.steps = [self.problem]

# %% ../nbs/00_core.ipynb 11
@dataclass
class LLM:
    client: AnthropicVertex
    model: str

# %% ../nbs/00_core.ipynb 13
lazy_system_p = """
        You are a helpful assistant that can help with math problems.
        You will be given a problem and a list of steps as context, the format will be:
                
        PROBLEM: <problem>
        STEPS: <steps>

        Your job is to complete the next step and only the next step in the problem-solving process. You should never give more than one step.
        If you evaluate that the problem is done, respond with "PROBLEM DONE"
        """

# %% ../nbs/00_core.ipynb 15
class LazyEvaluationClient:
    """The Lazy Evaluation Client"""
    def __init__(self, 
                 llm: LLM, # the language model to use, see `LLM` class
                 max_tokens: int = 100, # the maximum number of tokens to generate
                 state: Optional[LazyState] = None,
                 lazy_system_p: str = lazy_system_p
                ):
        self.model = llm.model
        self.client = llm.client
        self.max_tokens = max_tokens
        self.state = state
        self.lazy_system_p = lazy_system_p
        self.question_history = []

    def initalize_problem(self, problem: str) -> None:
        self.state = LazyState(problem=problem)
    
    def get_current_step(self) -> str:
        return self.state.steps[self.state.current_step]
    
    def get_next_step(self) -> str:
        if self.state is None:
            raise ValueError("Problem is not initialized, call initalize_problem first")

        messages: List[MessageParam] = [
            {
                "role": "user",
                "content": self.state.get_context()
            }
        ]

        response: Message = self.client.messages.create(
            system=self.lazy_system_p,
            model=self.model,
            messages=messages,
            max_tokens=self.max_tokens
        )
        next_step = response.content[0].text
        if next_step is not None:
            self.state.add_step(next_step.strip())
            return next_step.strip()
        else:
            raise ValueError("No next step found") 

# %% ../nbs/00_core.ipynb 27
@patch
def ask_question(self:LazyEvaluationClient, question:str) -> Message:
    """
    Allows the user to ask a question about the current step without affecting the model's ability to generate the next step.
    
    Args:
    question (str): The question the user wants to ask about the current step.
    
    Returns:
    str: The model's response to the question.
    """

    current_state = f"""
        System: {self.lazy_system_p}
        Problem: {self.state.problem}\n
        Context: {self.state.get_context()}
        Current step: {self.state.steps[self.state.current_step]}
    """
    prompt = f"""
        Question History: {self.question_history}
        Question: {question}\n
        Please answer the question without advancing to the next step.
        If you are asked to provide an example for a specific step, please provide an example that is not in the current context.
    """

    messages: List[MessageParam] = [
        {
            "role": "user",
            "content": prompt
        }
    ]

    response: Message = self.client.messages.create(
            system=current_state,
            model=self.model,
            messages=messages,
            max_tokens=self.max_tokens
        )
    self.question_history.append(question)
    self.question_history.append(response.content[0].text.strip())
    
    return response.content[0].text.strip()

# %% ../nbs/00_core.ipynb 32
@patch
def lazy(self: AnthropicVertex, problem: str) -> LazyEvaluationClient:
    """
    Entry point of the LazyLM Framework for the `AnthropicVertex` client API
    """
    state = LazyState(problem=problem)
    llm = LLM(client=self, model=model)
    return LazyEvaluationClient(llm=llm, state=state)
